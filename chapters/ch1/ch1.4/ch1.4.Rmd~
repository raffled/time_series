---
title: "Chapter 1 Continued"
output: html_document
header-includes:
   - \usepackage{amsmath}
---

## 1.4 Measures of Dependence: Autocorrelation and Cross-Correlation

A complete time series can be described as a collection of $n$ random
variables at arbitrary integer time points $t_1, t_2, \ldots, t_n$
(for positive integer $n$) has the joint CDF:

$$
	F(c_1, c_2, \ldots c_n) = P(x_{t_1} \le c_1, x_{t_2} \le c2,
		\ldots, x_{t_n} \le c_n)
$$

Obviously this multivariate CDF is not easily written except in the
case of the multivariate normal. Even when it is, visualizing
high-dimensional space is itself unwieldy.  Instead, consider the
marginal CDFs and PDFs:

$$ F_t(x) = P\{x_t \le x\} $$

and

$$ f_t(x) = \frac{\partial F_t(x)}{\partial x} $$

We also have the usual expected value, or mean, function:

$$ \mu_{xt} = E(x_t) = \int_{-\infty}^\infty xf_t(x) dx$$

### Example 1.13 Mean Function of a Moving Average Series
For white noise $w_t$, $\mu_{wt} = E(w_t) = 0$ for all $t$.
If we smooth as we did earlier,

$$v_t = \frac{1}{3}\sum_{j = t - 1}^{t + 1} w_j$$

averaging the point and its
nearest neighbors, the mean of the smoother is also 0:

$$
	\mu_{vt} = E(v_t) = \frac{1}{3}\left[
		E(w_{t - 1}) + E(w_t) + E(w_{t + 1})\right] = 0
$$

### Example 1.14 Mean Function of a Random Walk with Drift
Consider the random walk with drift model:

$$x_t = \delta t + \sum_{j=1}^t wj, \quad\quad t = 1, 2, \ldots$$

Because $E(w_t) = 0\; \forall\; t$, and $\delta$ is a constant, it is
clear that:

$$\mu_{xt} = E(x_t) = \delta t + \sum_{j=1}^t E(w_j) = \delta t$$

which is just a straight line with slope $\delta$. Typically a random
walk with drift can be compared to its mean function (as we did
earlier).

### Example 1.15 Mean Function of a Signal Plus Noise
In many applications, data are generated by a fixed signal waveform
superimposed on a noise process w/ mean 0. Because of that, we have

$$
\begin{aligned}
	\mu_{xt} = E(x_t) & = E\left[A\cos(2\pi \omega t + \phi) + w_t\right] \\
	& = A\cos(2\pi \omega t + \phi) + E\left[w_t\right] \\
	& = A\cos(2\pi \omega t + \phi)\\
\end{aligned}
$$

which is just the cosine wave.

From here, we can address the dependence between adjacent values $x_s$
and $x_t$ using covariance and correlation.

## Autocovariance
The autocovariance function is the second moment product

$$
	\gamma_x(s, t) = \text{cov}(x_s, x_t) =
		E\left[(x_s - \mu_s)(x_t - \mu_t)\right]
$$

Note that $\gamma_x(s, t) = \gamma_x(t, s)$ for all time points $s$
and $t$. We used autocovariance as a measure of the *linear*
dependence between two points on the same series observed at different
times.

Very smooth series has autocovariance functions that stay large when
$s$ and $t$ are distant, whereas choppy series have autocovariance
function that are near zero for time points that are far away.

The autocovariance is the average cross-product relative to the joint
CDF $F(x_s, x_t)$. From classical statistics, if $\gamma_x(s, t) = 0$,
then $x_s$ and $x_t$ are not linearly related (but may be non-linearly
related). If $x_s$ and $x_t$ are bivariate normal, $\gamma_x(s, t) =
0$ means that they are independent.

For $s = t$, the autocovariance is equal to the variance.

### Autocovariance of White Noise.
A white noise series $w_t$ has $E(x_t) = 0$ and

$$
	\gamma_w(s, t) = \text{cov}(w_s, w_t) =
	\begin{cases}
		\sigma^2_w & s = t \\
		0 & s \ne t \\
	\end{cases}
$$

### Autocovariance of a Moving Average
Using a 3-point moving average,

$$ v_t = \frac{1}{3}\sum_{j = t-1}^{t + 1} w_j $$

we have the covariance function,

$$
	\gamma_v(s, t) = \mbox{cov}(v_s, v_t) =
	\mbox{cov}\left{\frac{1}{3}(w_{s-1} + w_s + w_{s+1}),
		\frac{1}{3}(w_{t-1} + w_t + w_{t+1})\right}
$$		
	
