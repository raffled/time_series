<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>1.4 Measures of Dependence: Autocorrelation and Cross-Correlation</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h2>1.4 Measures of Dependence: Autocorrelation and Cross-Correlation</h2>

<p>A complete time series can be described as a collection of \(n\) random
variables at arbitrary integer time points \(t_1, t_2, \ldots, t_n\)
(for positive integer \(n\)) has the joint CDF:</p>

<p>\[
    F(c_1, c_2, \ldots c_n) = P(x_{t_1} \le c_1, x_{t_2} \le c2,
        \ldots, x_{t_n} \le c_n)
\]</p>

<p>Obviously this multivariate CDF is not easily written except in the
case of the multivariate normal. Even when it is, visualizing
high-dimensional space is itself unwieldy.  Instead, consider the
marginal CDFs and PDFs:</p>

<p>\[ F_t(x) = P\{x_t \le x\} \]</p>

<p>and</p>

<p>\[ f_t(x) = \frac{\partial F_t(x)}{\partial x} \]</p>

<p>We also have the usual expected value, or mean, function:</p>

<p>\[ \mu_{xt} = E(x_t) = \int_{-\infty}^\infty xf_t(x) dx\]</p>

<h3>Example 1.13 Mean Function of a Moving Average Series</h3>

<p>For white noise \(w_t\), \(\mu_{wt} = E(w_t) = 0\) for all \(t\).
If we smooth as we did earlier,</p>

<p>\[v_t = \frac{1}{3}\sum_{j = t - 1}^{t + 1} w_j\]</p>

<p>averaging the point and its
nearest neighbors, the mean of the smoother is also 0:</p>

<p>\[
    \mu_{vt} = E(v_t) = \frac{1}{3}\left[
        E(w_{t - 1}) + E(w_t) + E(w_{t + 1})\right] = 0
\]</p>

<h3>Example 1.14 Mean Function of a Random Walk with Drift</h3>

<p>Consider the random walk with drift model:</p>

<p>\[x_t = \delta t + \sum_{j=1}^t wj, \quad\quad t = 1, 2, \ldots\]</p>

<p>Because \(E(w_t) = 0\; \forall\; t\), and \(\delta\) is a constant, it is
clear that:</p>

<p>\[\mu_{xt} = E(x_t) = \delta t + \sum_{j=1}^t E(w_j) = \delta t\]</p>

<p>which is just a straight line with slope \(\delta\). Typically a random
walk with drift can be compared to its mean function (as we did
earlier).</p>

<h3>Example 1.15 Mean Function of a Signal Plus Noise</h3>

<p>In many applications, data are generated by a fixed signal waveform
superimposed on a noise process w/ mean 0. Because of that, we have</p>

<p>\[
\begin{aligned}
    \mu_{xt} = E(x_t) & = E\left[A\cos(2\pi \omega t + \phi) + w_t\right] \\
    & = A\cos(2\pi \omega t + \phi) + E\left[w_t\right] \\
    & = A\cos(2\pi \omega t + \phi)\\
\end{aligned}
\]</p>

<p>which is just the cosine wave.</p>

<p>From here, we can address the dependence between adjacent values \(x_s\)
and \(x_t\) using covariance and correlation.</p>

<h2>Autocovariance</h2>

<p>The autocovariance function is the second moment product</p>

<p>\[
    \gamma_x(s, t) = \text{cov}(x_s, x_t) =
        E\left[(x_s - \mu_s)(x_t - \mu_t)\right]
\]</p>

<p>Note that \(\gamma_x(s, t) = \gamma_x(t, s)\) for all time points \(s\)
and \(t\). We used autocovariance as a measure of the <em>linear</em>
dependence between two points on the same series observed at different
times.</p>

<p>Very smooth series has autocovariance functions that stay large when
\(s\) and \(t\) are distant, whereas choppy series have autocovariance
function that are near zero for time points that are far away.</p>

<p>The autocovariance is the average cross-product relative to the joint
CDF \(F(x_s, x_t)\). From classical statistics, if \(\gamma_x(s, t) = 0\),
then \(x_s\) and \(x_t\) are not linearly related (but may be non-linearly
related). If \(x_s\) and \(x_t\) are bivariate normal, \(\gamma_x(s, t) =
0\) means that they are independent.</p>

<p>For \(s = t\), the autocovariance is equal to the variance.</p>

<h3>Autocovariance of White Noise.</h3>

<p>A white noise series \(w_t\) has \(E(x_t) = 0\) and</p>

<p>\[
    \gamma_w(s, t) = \text{cov}(w_s, w_t) =
    \begin{cases}
        \sigma^2_w & s = t \\
        0 & s \ne t \\
    \end{cases}
\]</p>

<h3>Autocovariance of a Moving Average</h3>

<p>Using a 3-point moving average,</p>

<p>\[ v_t = \frac{1}{3}\sum_{j = t-1}^{t + 1} w_j \]</p>

<p>we have the covariance function,</p>

<p>\[
    \gamma_v(s, t) = \mbox{cov}(v_s, v_t) =
    \mbox{cov}\left[\frac{1}{3}(w_{s-1} + w_s + w_{s+1}),
        \frac{1}{3}(w_{t-1} + w_t + w_{t+1})\right]
\]      </p>

<p>When \(s = t\),</p>

<p>\[\begin{aligned}
    \gamma_v(t, t) &=
        \frac{1}{9}\mbox{cov}\left[\frac{1}{3}(w_{t-1} + w_t + w_{t+1}),
            \frac{1}{3}(w_{t-1} + w_t + w_{t+1})\right] \\
        &= \frac{1}{9}\left[\mbox{cov}(w_{t-1}, w_{t-1}) +
                            \mbox{cov}(w_{t}, w_{t}) +
                            \mbox{cov}(w_{t+1}, w_{t+1})
                        \right] \\
        &= \frac{3}{9}\sigma_w^2
\end{aligned}\]</p>

<p>as one would expect. However, if \(s = t+1\), we have</p>

<p>\[\begin{aligned}
    \gamma_v(t + 1, t) &=
        \frac{1}{9}\mbox{cov}\left[
            (w_t + w_{t+1} + w_{t+2}),
            (w_{t-1} + w_t + w_{t+1})
        \right] \\
    &= \frac{1}{9}\left[
            \mbox{cov}(w_t, w_t) +
            \mbox{cov}(w_{t+1}, w_{t+1})
        \right] \\
    &= \frac{2}{9}\sigma_w^2
\end{aligned}\]</p>

<p>The other pairwise differences are similar, giving us:</p>

<p>\[
    \gamma_v(s, t) = \begin{cases}
        \frac{3}{9}\sigma_w^2 & s = t \\
        \frac{2}{9}\sigma_w^2 & | s - t | = 1 \\
        \frac{1}{9}\sigma_w^2 & |s - t| = 2\\
        0 & | s - t | > 2
    \end{cases}
\]</p>

<p>So the smoothing creates an autocovariance function which <em>decreases</em>
as points become further apart, and disappears when the lag is larger
than the number of points being smoothed.</p>

</body>

</html>
